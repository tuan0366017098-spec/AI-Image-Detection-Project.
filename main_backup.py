from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import torch
from PIL import Image
import io
import os
import sys

# Th√™m th∆∞ m·ª•c hi·ªán t·∫°i v√†o path ƒë·ªÉ import
sys.path.append(os.path.dirname(__file__))

app = FastAPI(title="AI vs Real Image Detector API")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global variables
model = None
model_loaded = False
model_type = "none"


def load_real_model():
    """Load model th·∫≠t t·ª´ file"""
    global model, model_loaded, model_type

    try:
        print("üîç ƒêang t√¨m ki·∫øm model th·∫≠t...")

        # C√°c v·ªã tr√≠ c√≥ th·ªÉ c√≥ model
        possible_paths = [
            "models/model_best.pth",
            "models/best_model.pth",
            "models/checkpoint.pth",
            "models/ai_real_classifier.h5",
            "../Training-models/src/checkpoints/model_best.pth",
            "../Training-models/models/model_best.pth"
        ]

        model_path = None
        for path in possible_paths:
            if os.path.exists(path):
                model_path = path
                print(f"‚úÖ T√¨m th·∫•y model: {path}")
                break

        if not model_path:
            print("‚ùå Kh√¥ng t√¨m th·∫•y file model n√†o!")
            return False

        # Import model_loader
        try:
            from model_loader import load_model, predict_ai_image
            print("‚úÖ Import model_loader th√†nh c√¥ng")
        except ImportError as e:
            print(f"‚ùå L·ªói import model_loader: {e}")
            return False

        # Load model
        print(f"üîÑ ƒêang load model t·ª´: {model_path}")
        model = load_model(model_path)

        # Ki·ªÉm tra xem model c√≥ ph·∫£i dummy kh√¥ng
        if hasattr(model, '__class__') and 'DummyModel' in str(model.__class__):
            print("‚ùå Model ƒë∆∞·ª£c load l√† DUMMY MODEL")
            return False
        else:
            print("‚úÖ Load MODEL TH·∫¨T th√†nh c√¥ng!")
            model_loaded = True
            model_type = "real"
            return True

    except Exception as e:
        print(f"‚ùå L·ªói khi load model: {e}")
        import traceback
        traceback.print_exc()
        return False


def dummy_predict(image_bytes, filename):
    """Dummy model d·ª± ph√≤ng"""
    return {
        "is_ai_generated": len(filename) % 2 == 0,
        "confidence_score": 0.75,
        "label": "AI Generated" if len(filename) % 2 == 0 else "Real Image"
    }


@app.on_event("startup")
async def startup_event():
    print("üöÄ Kh·ªüi ƒë·ªông AI Image Detector...")

    # Th·ª≠ load model th·∫≠t
    if load_real_model():
        print("üéØ H·ªÜ TH·ªêNG ƒêANG S·ª¨ D·ª§NG MODEL TH·∫¨T")
    else:
        print("‚ö†Ô∏è  S·ª¨ D·ª§NG DUMMY MODEL (kh√¥ng t√¨m th·∫•y model th·∫≠t)")
        model_type = "dummy"


@app.get("/")
def read_root():
    model_status = "REAL MODEL" if model_loaded else "DUMMY MODEL"
    return {
        "message": f"AI Image Detector API - ƒêang s·ª≠ d·ª•ng: {model_status}",
        "model_loaded": model_loaded,
        "model_type": model_type
    }


@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "model_loaded": model_loaded,
        "model_type": model_type,
        "message": "ƒêang s·ª≠ d·ª•ng model th·∫≠t" if model_loaded else "ƒêang s·ª≠ d·ª•ng dummy model"
    }


@app.post("/predict")
async def predict_image(file: UploadFile = File(...)):
    if not file.content_type.startswith('image/'):
        raise HTTPException(status_code=400, detail="File must be an image")

    try:
        contents = await file.read()

        if len(contents) > 10 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="File too large")

        # S·ª≠ d·ª•ng model th·∫≠t n·∫øu c√≥
        if model_loaded:
            try:
                from model_loader import predict_ai_image
                result = predict_ai_image(contents)
                model_used = "REAL"
            except Exception as e:
                print(f"‚ùå L·ªói khi d·ª± ƒëo√°n v·ªõi model th·∫≠t: {e}")
                result = dummy_predict(contents, file.filename)
                model_used = "DUMMY (fallback)"
        else:
            result = dummy_predict(contents, file.filename)
            model_used = "DUMMY"

        if "error" in result:
            raise HTTPException(status_code=500, detail=result["error"])

        # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£
        is_ai = result["is_ai_generated"]
        confidence = result["confidence_score"]
        class_name = "·∫¢NH DO AI T·∫†O" if is_ai else "·∫¢NH TH·∫¨T"

        print(f"üìä D·ª± ƒëo√°n ({model_used}): {class_name} - ƒê·ªô tin c·∫≠y: {confidence:.2%}")

        return JSONResponse({
            "prediction": 1 if is_ai else 0,
            "class_name": class_name,
            "confidence": confidence,
            "filename": file.filename,
            "model_used": model_used,
            "message": f"K·∫øt qu·∫£ t·ª´ {model_used}: {class_name} ({(confidence * 100):.1f}%)"
        })

    except Exception as e:
        print(f"‚ùå L·ªói d·ª± ƒëo√°n: {e}")
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω ·∫£nh: {str(e)}")


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)